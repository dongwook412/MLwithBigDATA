{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name_list = ['mnist', 'fashion_mnist', 'cifar10', 'cifar100', 'resisc45']\n",
    "class_list = [10, 10, 10, 100, 45]\n",
    "\n",
    "k = 0\n",
    "data_name = data_name_list[k]\n",
    "num_class = class_list[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "def normalize(dataset):    \n",
    "    image, label = tf.cast(dataset['image'], tf.float32) / 255.0, dataset['label']\n",
    "    return image, label\n",
    "\n",
    "train_data, test_data = tfds.load(\n",
    "    data_name, \n",
    "    #split=(tfds.Split.TRAIN, tfds.Split.TEST), \n",
    "    shuffle_files=False, as_supervised=False)\n",
    "\n",
    "iterator_train = iter(train_data.map(normalize).batch(50))\n",
    "iterator_test = iter(test_data.map(normalize))\n",
    "\n",
    "train = [] # [i][j] i = i번째 배치, j = 0은 이미지, 1은 라벨 \n",
    "test = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        train.append(next(iterator_train))\n",
    "    except StopIteration:\n",
    "        break\n",
    "while True:\n",
    "    try:\n",
    "        test.append(next(iterator_test))\n",
    "    except StopIteration:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = [] # [i][j] i= i번째 배치, j= j번째 이미지\n",
    "train_label = []\n",
    "\n",
    "test_image = [] # [i][j] i= i번째 배치, j= j번째 이미지\n",
    "test_label = []\n",
    "\n",
    "#train\n",
    "for i in range(len(train)):\n",
    "    \n",
    "    target_train = train[i][1].numpy()\n",
    "    \n",
    "    encoding = np.eye(num_class)[target_train]\n",
    "    train_image.append(train[i][0].numpy().reshape((-1,32*32*3)))\n",
    "    train_label.append(encoding)\n",
    "\n",
    "#test\n",
    "for i in range(len(test)):    \n",
    "    target_test = test[i][1].numpy()\n",
    "    \n",
    "    encoding = np.eye(num_class)[target_test]\n",
    "    test_image.append(test[i][0].numpy().reshape((-1,32*32*3)))\n",
    "    test_label.append(encoding)\n",
    "    \n",
    "test_image = np.array(test_image).reshape(len(test_image), test_image[0].shape[1])\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, (50, 3072))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_image), train_image[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, (3072,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_image), test_image[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test_label).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\data\\\\'+data_name+'_train_image', train_image)\n",
    "np.save('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\data\\\\'+data_name+'_train_label', train_label)\n",
    "np.save('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\data\\\\'+data_name+'_test_image', test_image)\n",
    "np.save('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\data\\\\'+data_name+'_test_label', test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_image = np.load('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\image_'+data_name+'.npy')\n",
    "n_label = np.load('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\label_'+data_name+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(n_image),len(n_image[0]), len(n_image[0][1]), len(n_image[0][0][0]), len(n_image[0][0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 is 80.000000\n",
      "P2 is 92.000000\n",
      "P3 is 99.100000\n",
      "P4 is 93.000000\n",
      "Saving graph to: C:\\Users\\Yang\\AppData\\Local\\Temp\\tmp7__dao3o\n",
      "step 0, training accuracy 0.12\n",
      "step 100, training accuracy 0.94\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 0.86\n",
      "step 400, training accuracy 0.96\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 0.98\n",
      "step 700, training accuracy 0.98\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 1\n",
      "test accuracy 0.9845\n",
      "percentile 0.13910980522632602\n",
      "percentile 0.15807356655597687\n",
      "percentile 0.19487644244730482\n",
      "percentile 0.16286328852176668\n",
      "step 0, training accuracy 0.98\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 0.94\n",
      "step 300, training accuracy 0.98\n",
      "step 400, training accuracy 0.98\n",
      "percentile 0.14124430716037753\n",
      "percentile 0.15557461798191072\n",
      "percentile 0.17708150936663178\n",
      "percentile 0.16698487162590028\n",
      "test accuracy 0.9854\n",
      "step 0, training accuracy 0.98\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 0.98\n",
      "step 400, training accuracy 0.98\n",
      "percentile 0.2265529990196228\n",
      "percentile 0.24442820429801956\n",
      "percentile 0.28041999179124855\n",
      "percentile 0.27708340466022496\n",
      "test accuracy 0.9857\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 0.94\n",
      "step 400, training accuracy 1\n",
      "percentile 0.31340221762657167\n",
      "percentile 0.3272002029418945\n",
      "percentile 0.35571707066893593\n",
      "percentile 0.3855627003312112\n",
      "test accuracy 0.9893\n",
      "step 0, training accuracy 0.94\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 1\n",
      "percentile 0.3249097287654877\n",
      "percentile 0.36044164061546324\n",
      "percentile 0.38751893964409834\n",
      "percentile 0.4587660270929337\n",
      "test accuracy 0.9872\n",
      "step 0, training accuracy 0.96\n",
      "step 100, training accuracy 0.98\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 0.98\n",
      "step 400, training accuracy 1\n",
      "percentile 0.39030207395553596\n",
      "percentile 0.3947736263275149\n",
      "percentile 0.4025098095834256\n",
      "percentile 0.5053149670362473\n",
      "test accuracy 0.9844\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 0.98\n",
      "step 400, training accuracy 0.96\n",
      "percentile 0.4174243569374085\n",
      "percentile 0.41692805528640753\n",
      "percentile 0.4054261847734454\n",
      "percentile 0.5763887882232667\n",
      "test accuracy 0.988\n",
      "step 0, training accuracy 0.98\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 0.98\n",
      "step 400, training accuracy 1\n",
      "percentile 0.41674495339393625\n",
      "percentile 0.4034493160247804\n",
      "percentile 0.3886370790600782\n",
      "percentile 0.592580201625824\n",
      "test accuracy 0.9883\n",
      "step 0, training accuracy 0.98\n",
      "step 100, training accuracy 0.98\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 0.98\n",
      "step 400, training accuracy 0.98\n",
      "percentile 0.4640497446060181\n",
      "percentile 0.3993754875659943\n",
      "percentile 0.3748529612421989\n",
      "percentile 0.6007899171113973\n",
      "test accuracy 0.9892\n",
      "step 0, training accuracy 0.98\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 0.96\n",
      "step 400, training accuracy 1\n",
      "percentile 0.4797618627548218\n",
      "percentile 0.3954162514209748\n",
      "percentile 0.3603493717312816\n",
      "percentile 0.6075729674100877\n",
      "test accuracy 0.9862\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 0.98\n",
      "percentile 0.4774931430816651\n",
      "percentile 0.36730693459510805\n",
      "percentile 0.33868683403730393\n",
      "percentile 0.6037917339801789\n",
      "test accuracy 0.9898\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 0.98\n",
      "percentile 0.5068216323852541\n",
      "percentile 0.3545851135253908\n",
      "percentile 0.3312135759294035\n",
      "percentile 0.5868593770265581\n",
      "test accuracy 0.9901\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 0.98\n",
      "percentile 0.5296257972717286\n",
      "percentile 0.33227928638458254\n",
      "percentile 0.319673258394003\n",
      "percentile 0.5690468430519106\n",
      "test accuracy 0.9902\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 1\n",
      "percentile 0.5213399410247804\n",
      "percentile 0.31427789211273205\n",
      "percentile 0.3129427678883078\n",
      "percentile 0.5435365021228791\n",
      "test accuracy 0.9881\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 0.98\n",
      "step 400, training accuracy 1\n",
      "percentile 0.5465113043785097\n",
      "percentile 0.29962469458580027\n",
      "percentile 0.30721659132838325\n",
      "percentile 0.5165512382984163\n",
      "test accuracy 0.9906\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 1\n",
      "percentile 0.5466767549514772\n",
      "percentile 0.29307109475135806\n",
      "percentile 0.3014370026886467\n",
      "percentile 0.4939408499002458\n",
      "test accuracy 0.9872\n",
      "step 0, training accuracy 0.98\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 1\n",
      "percentile 0.5594950318336487\n",
      "percentile 0.27904783129692085\n",
      "percentile 0.2938431117832666\n",
      "percentile 0.47129664093256\n",
      "test accuracy 0.9907\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 1\n",
      "percentile 0.573090410232544\n",
      "percentile 0.2748952257633209\n",
      "percentile 0.2898185659646992\n",
      "percentile 0.4482733678817752\n",
      "test accuracy 0.9913\n",
      "step 0, training accuracy 0.98\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 1\n",
      "percentile 0.5944469094276429\n",
      "percentile 0.2688710963726044\n",
      "percentile 0.28335565820336384\n",
      "percentile 0.42574126899242404\n",
      "test accuracy 0.9903\n",
      "step 0, training accuracy 1\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 1\n",
      "step 300, training accuracy 0.92\n",
      "step 400, training accuracy 1\n",
      "percentile 0.5841199398040772\n",
      "percentile 0.26579499483108526\n",
      "percentile 0.27637475955486335\n",
      "percentile 0.4098115390539173\n",
      "test accuracy 0.9887\n",
      "step 0, training accuracy 0.96\n",
      "step 100, training accuracy 1\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 1\n",
      "step 400, training accuracy 1\n",
      "percentile 0.6143157005310059\n",
      "percentile 0.2601363182067871\n",
      "percentile 0.2675761563777925\n",
      "percentile 0.3980568346381188\n",
      "test accuracy 0.9872\n",
      "at weight conv1/W_conv1\n",
      "before pruning #non zero parameters 500\n",
      "percentile 0.23655965328216555\n",
      "pruned 400\n",
      "after prunning #non zero parameters 100\n",
      "at weight conv2/W_conv2\n",
      "before pruning #non zero parameters 25000\n",
      "percentile 0.11670302629470826\n",
      "pruned 23000\n",
      "after prunning #non zero parameters 2000\n",
      "at weight fc1/W_fc1\n",
      "before pruning #non zero parameters 400000\n",
      "percentile 0.10505181773752031\n",
      "pruned 396400\n",
      "after prunning #non zero parameters 3600\n",
      "at weight fc2/W_fc2\n",
      "before pruning #non zero parameters 5000\n",
      "percentile 0.17970015898346947\n",
      "pruned 4650\n",
      "after prunning #non zero parameters 350\n",
      "checking space dictionary\n",
      "dict_keys(['conv1/W_conv1', 'conv2/W_conv2', 'fc1/W_fc1', 'fc2/W_fc2'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to convert 'y' to a tensor and failed. Error: None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    511\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 208\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None values not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m     \u001b[1;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    523\u001b[0m               observed = ops.internal_convert_to_tensor(\n\u001b[1;32m--> 524\u001b[1;33m                   values, as_ref=input_arg.is_ref).dtype.name\n\u001b[0m\u001b[0;32m    525\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 208\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"None values not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m     \u001b[1;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: None values not supported.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-83fc195c04f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    282\u001b[0m                       help='Directory for storing input data')\n\u001b[0;32m    283\u001b[0m   \u001b[0mFLAGS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 284\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0munparsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-83fc195c04f7>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"checking space dictionary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict_nzidx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_prune_on_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict_nzidx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[0mapply_gradient_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\admm-pruning-master\\admm-pruning-master\\tensorflow-mnist-code\\prune_utility.py\u001b[0m in \u001b[0;36mapply_prune_on_grads\u001b[1;34m(grads_and_vars, dict_nzidx)\u001b[0m\n\u001b[0;32m     62\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\":0\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mnzidx_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnzidx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnzidx_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m       \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"math.multiply\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiply\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   5040\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5041\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m-> 5042\u001b[1;33m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m   5043\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5044\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    526\u001b[0m               raise ValueError(\n\u001b[0;32m    527\u001b[0m                   \u001b[1;34m\"Tried to convert '%s' to a tensor and failed. Error: %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m                   (input_name, err))\n\u001b[0m\u001b[0;32m    529\u001b[0m             prefix = (\"Input '%s' of '%s' Op has type %s that does not match\" %\n\u001b[0;32m    530\u001b[0m                       (input_name, op_type_name, observed))\n",
      "\u001b[1;31mValueError\u001b[0m: Tried to convert 'y' to a tensor and failed. Error: None values not supported."
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"A deep MNIST classifier using convolutional layers.\n",
    "\n",
    "See extensive documentation at\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n",
    "\"\"\"\n",
    "# Disable linter warnings to maintain consistency with tutorial.\n",
    "# pylint: disable=invalid-name\n",
    "# pylint: disable=g-bad-import-order\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "from model import create_model\n",
    "from solver import create_admm_solver\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from prune_utility import apply_prune_on_grads,apply_prune,get_configuration,projection\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#tf.disable_eager_execution()\n",
    "\n",
    "FLAGS = None\n",
    "# pruning ratio\n",
    "\n",
    "data_name_list = ['mnist', 'fashion_mnist', 'Cifar10', 'Cifar100']\n",
    "\n",
    "data_name = data_name_list[0]\n",
    "num_class = 10\n",
    "\n",
    "prune_configuration = get_configuration()\n",
    "dense_w = {}\n",
    "P1 = prune_configuration.P1\n",
    "P2 = prune_configuration.P2\n",
    "P3 = prune_configuration.P3\n",
    "P4 = prune_configuration.P4\n",
    "\n",
    "prune_configuration.display()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  # Import data\n",
    "  train_image = np.load('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\data\\\\'+data_name+'_train_image'+'.npy')\n",
    "  train_label = np.load('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\data\\\\'+data_name+'_train_label'+'.npy')\n",
    "  test_image = np.load('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\data\\\\'+data_name+'_test_image'+'.npy')\n",
    "  test_label = np.load('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\data\\\\'+data_name+'_test_label'+'.npy')\n",
    "\n",
    "  model = create_model()\n",
    "  x = model.x\n",
    "  y_ = model.y_\n",
    "  cross_entropy = model.cross_entropy\n",
    "  layers = model.layers\n",
    "  logits = model.logits\n",
    "  solver = create_admm_solver(model)\n",
    "  keep_prob = model.keep_prob\n",
    "  train_step = solver.train_step\n",
    "  train_step1 = solver.train_step1\n",
    "  \n",
    "  W_conv1 = model.W_conv1\n",
    "  W_conv2 = model.W_conv2\n",
    "  W_fc1 = model.W_fc1\n",
    "  W_fc2 = model.W_fc2\n",
    "  \n",
    "  A = solver.A\n",
    "  B = solver.B\n",
    "  C = solver.C\n",
    "  D = solver.D\n",
    "  E = solver.E\n",
    "  F = solver.F\n",
    "  G = solver.G\n",
    "  H = solver.H\n",
    "    \n",
    "\n",
    "  # 첫 번째 stop condition\n",
    "  stop_W_Z_Conv1 = [] \n",
    "  stop_W_Z_Conv2 = [] \n",
    "  stop_W_Z_FC1 = []\n",
    "  stop_W_Z_FC2 = []\n",
    "  stop_W_Z = [] #전체 값  \n",
    "    \n",
    "  # 두 번째 stop condition\n",
    "  stop_Z_Z_Conv1 = [] \n",
    "  stop_Z_Z_Conv2 = [] \n",
    "  stop_Z_Z_FC1 = [] \n",
    "  stop_Z_Z_FC2 = [] \n",
    "  stop_W_Z = [] #전체 값  \n",
    "    \n",
    "    \n",
    "  my_trainer = tf.train.AdamOptimizer(1e-3)\n",
    "  grads = my_trainer.compute_gradients(cross_entropy)\n",
    "    \n",
    "  with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "  accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "  graph_location = tempfile.mkdtemp()\n",
    "  print('Saving graph to: %s' % graph_location)\n",
    "  train_writer = tf.summary.FileWriter(graph_location)\n",
    "  train_writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(1000):\n",
    "        j = random.randrange(0, len(train_image), 1)\n",
    "        batch = []\n",
    "        batch.append(train_image[j])\n",
    "        batch.append(train_label[j])\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "        x: test_image, y_: test_label, keep_prob: 1.0}))\n",
    "    \n",
    "    pretrained_weight = np.array([sess.run(W_conv1), sess.run(W_conv2), sess.run(W_fc1), sess.run(W_fc2)])\n",
    "    \n",
    "    # pretrain된 weight 저장 \n",
    "    np.save('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\weight\\\\'+'pretrained_'+data_name, pretrained_weight)\n",
    "    \n",
    "#     # pretrain 모델 저장\n",
    "#     saver = tf.train.Saver()\n",
    "#     saver.save(sess, 'C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\model\\\\'+data_name+'_pretrained')\n",
    "    \n",
    "    Z1 = sess.run(W_conv1)\n",
    "    Z1 = projection(Z1, percent=P1)\n",
    "\n",
    "    U1 = np.zeros_like(Z1)\n",
    "\n",
    "    Z2 = sess.run(W_conv2)\n",
    "    Z2 = projection(Z2, percent=P2)\n",
    "\n",
    "    U2 = np.zeros_like(Z2)\n",
    "\n",
    "    Z3 = sess.run(W_fc1)\n",
    "    Z3 = projection(Z3, percent=P3)\n",
    "\n",
    "    U3 = np.zeros_like(Z3)\n",
    "\n",
    "    Z4 = sess.run(W_fc2)\n",
    "    Z4 = projection(Z4, percent=P4)\n",
    "\n",
    "    U4 = np.zeros_like(Z4)\n",
    "    \n",
    "    for j in range(20):\n",
    "        for i in range(500):\n",
    "            j = random.randrange(0, len(train_image), 1)\n",
    "            batch = []\n",
    "            batch.append(train_image[j])\n",
    "            batch.append(train_label[j])\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                    x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "                print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "            train_step1.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0, A:Z1, B:U1, C:Z2, D:U2, E:Z3, F:U3, G:Z4, H:U4})\n",
    "        \n",
    "        #Z k번째값 저장\n",
    "        Z_k = [Z1, Z2, Z3, Z4]\n",
    "        \n",
    "        Z1 = sess.run(W_conv1) + U1\n",
    "        Z1 = projection(Z1, percent=P1)\n",
    "\n",
    "        U1 = U1 + sess.run(W_conv1) - Z1\n",
    "\n",
    "        Z2 = sess.run(W_conv2) + U2\n",
    "        Z2 = projection(Z2, percent=P2)\n",
    "\n",
    "        U2 = U2 + sess.run(W_conv2) - Z2\n",
    "\n",
    "        Z3 = sess.run(W_fc1) + U3\n",
    "        Z3 = projection(Z3, percent=P3)\n",
    "\n",
    "        U3 = U3 + sess.run(W_fc1) - Z3\n",
    "\n",
    "        Z4 = sess.run(W_fc2) + U4\n",
    "        Z4 = projection(Z4, percent=P4)\n",
    "\n",
    "        U4 = U4 + sess.run(W_fc2) - Z4\n",
    "        \n",
    "        \n",
    "        print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "            x: test_image, y_: test_label, keep_prob: 1.0}))\n",
    "        \n",
    "        #stop condition 값 저장 \n",
    "        stop_W_Z_Conv1.append(LA.norm(sess.run(W_conv1) - Z1))\n",
    "        stop_W_Z_Conv2.append(LA.norm(sess.run(W_conv2) - Z2))\n",
    "        stop_W_Z_FC1.append(LA.norm(sess.run(W_fc1) - Z3))\n",
    "        stop_W_Z_FC2.append(LA.norm(sess.run(W_fc2) - Z4))\n",
    "        \n",
    "        \n",
    "        stop_Z_Z_Conv1.append(LA.norm(Z_k[0]-Z1))\n",
    "        stop_Z_Z_Conv2.append(LA.norm(Z_k[1]-Z2))\n",
    "        stop_Z_Z_FC1.append(LA.norm(Z_k[2]-Z3))\n",
    "        stop_Z_Z_FC2.append(LA.norm(Z_k[3]-Z4))\n",
    "        \n",
    "#         print(LA.norm(sess.run(W_conv1) - Z1))\n",
    "#         print(LA.norm(sess.run(W_conv2) - Z2))\n",
    "#         print(LA.norm(sess.run(W_fc1) - Z3))\n",
    "#         print(LA.norm(sess.run(W_fc2) - Z4))\n",
    "\n",
    "    \n",
    "    \n",
    "    stop_W_Z = [stop_W_Z_Conv1, stop_W_Z_Conv2, stop_W_Z_FC1, stop_W_Z_FC2]\n",
    "    stop_Z_Z = [stop_Z_Z_Conv1, stop_Z_Z_Conv2, stop_Z_Z_FC1, stop_Z_Z_FC2]\n",
    "    \n",
    "    np.save('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\convergence\\\\'+data_name+'_stopWZ', stop_W_Z)\n",
    "    np.save('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\convergence\\\\'+data_name+'_stopZZ', stop_Z_Z)\n",
    "    \n",
    "    \n",
    "    dense_w['conv1/W_conv1'] = W_conv1\n",
    "    dense_w['conv2/W_conv2'] = W_conv2\n",
    "    dense_w['fc1/W_fc1'] = W_fc1\n",
    "    dense_w['fc2/W_fc2'] = W_fc2\n",
    "    \n",
    "    dict_nzidx = apply_prune(dense_w,sess)\n",
    "    admm_weight = np.array([sess.run(W_conv1), sess.run(W_conv2), sess.run(W_fc1), sess.run(W_fc2)])\n",
    "    np.save('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\weight\\\\'+'admm_'+data_name, admm_weight)\n",
    "    \n",
    "    \n",
    "    print (\"checking space dictionary\")\n",
    "    print (dict_nzidx.keys())\n",
    "    grads = apply_prune_on_grads(grads,dict_nzidx)\n",
    "    apply_gradient_op = my_trainer.apply_gradients(grads)\n",
    "    for var in tf.global_variables():\n",
    "                if tf.is_variable_initialized(var).eval() == False:\n",
    "                    sess.run(tf.variables_initializer([var]))\n",
    "    print (\"start retraining after pruning\")\n",
    "    for i in range(2000):\n",
    "        j = random.randrange(0, len(train_image), 1)\n",
    "        batch = []\n",
    "        batch.append(train_image[j])\n",
    "        batch.append(train_label[j])\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={\n",
    "                x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "        apply_gradient_op.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "    retrain_weight = np.array([sess.run(W_conv1), sess.run(W_conv2), sess.run(W_fc1), sess.run(W_fc2)])\n",
    "    np.save('C:\\\\Users\\\\Yang\\\\Desktop\\\\admm-pruning-master\\\\weight\\\\'+'retrained_'+data_name, retrain_weight)\n",
    "    \n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "          x: test_image, y_: test_label, keep_prob: 1.0}))\n",
    "    print(np.sum(sess.run(W_conv1)!=0))\n",
    "    print(np.sum(sess.run(W_conv2) != 0))\n",
    "    print(np.sum(sess.run(W_fc1) != 0))\n",
    "    print(np.sum(sess.run(W_fc2) != 0))\n",
    "    # do the saving.\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,\"./lenet_5_pruned_model.ckpt\")\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--data_dir', type=str,\n",
    "                      default='/tmp/tensorflow/mnist/input_data',\n",
    "                      help='Directory for storing input data')\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1 is 80.000000\n",
      "P2 is 92.000000\n",
      "P3 is 99.100000\n",
      "P4 is 93.000000\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tf.placeholder() is not compatible with eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-57ce677993c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    199\u001b[0m                       help='Directory for storing input data')\n\u001b[0;32m    200\u001b[0m   \u001b[0mFLAGS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munparsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0munparsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m    123\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-57ce677993c8>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\admm-pruning-master\\admm-pruning-master\\tensorflow-mnist-code\\model.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'entropy_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\admm-pruning-master\\admm-pruning-master\\tensorflow-mnist-code\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'conv1/W_conv1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'conv2/W_conv2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fc1/W_fc1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'fc2/W_fc2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;31m# Define loss and optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   2072\u001b[0m   \"\"\"\n\u001b[0;32m   2073\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2074\u001b[1;33m     raise RuntimeError(\"tf.placeholder() is not compatible with \"\n\u001b[0m\u001b[0;32m   2075\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   2076\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: tf.placeholder() is not compatible with eager execution."
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"A deep MNIST classifier using convolutional layers.\n",
    "\n",
    "See extensive documentation at\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n",
    "\"\"\"\n",
    "# Disable linter warnings to maintain consistency with tutorial.\n",
    "# pylint: disable=invalid-name\n",
    "# pylint: disable=g-bad-import-order\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import tempfile\n",
    "from model import create_model\n",
    "from solver import create_admm_solver\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from prune_utility import apply_prune_on_grads,apply_prune,get_configuration,projection\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "FLAGS = None\n",
    "# pruning ratio\n",
    "\n",
    "\n",
    "prune_configuration = get_configuration()\n",
    "dense_w = {}\n",
    "P1 = prune_configuration.P1\n",
    "P2 = prune_configuration.P2\n",
    "P3 = prune_configuration.P3\n",
    "P4 = prune_configuration.P4\n",
    "\n",
    "prune_configuration.display()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  # Import data\n",
    "  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "  model = create_model()\n",
    "  x = model.x\n",
    "  y_ = model.y_\n",
    "  cross_entropy = model.cross_entropy\n",
    "  layers = model.layers\n",
    "  logits = model.logits\n",
    "  solver = create_admm_solver(model)\n",
    "  keep_prob = model.keep_prob\n",
    "  train_step = solver.train_step\n",
    "  train_step1 = solver.train_step1\n",
    "  \n",
    "  W_conv1 = model.W_conv1\n",
    "  W_conv2 = model.W_conv2\n",
    "  W_fc1 = model.W_fc1\n",
    "  W_fc2 = model.W_fc2\n",
    "  \n",
    "  A = solver.A\n",
    "  B = solver.B\n",
    "  C = solver.C\n",
    "  D = solver.D\n",
    "  E = solver.E\n",
    "  F = solver.F\n",
    "  G = solver.G\n",
    "  H = solver.H\n",
    "\n",
    "  my_trainer = tf.train.AdamOptimizer(1e-3)\n",
    "  grads = my_trainer.compute_gradients(cross_entropy)\n",
    "    \n",
    "  with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_, 1))\n",
    "    correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "  accuracy = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "  graph_location = tempfile.mkdtemp()\n",
    "  print('Saving graph to: %s' % graph_location)\n",
    "  train_writer = tf.summary.FileWriter(graph_location)\n",
    "  train_writer.add_graph(tf.get_default_graph())\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(500):\n",
    "      batch = mnist.train.next_batch(50)\n",
    "      if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "      train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "        x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "\n",
    "    Z1 = sess.run(W_conv1)\n",
    "    Z1 = projection(Z1, percent=P1)\n",
    "\n",
    "    U1 = np.zeros_like(Z1)\n",
    "\n",
    "    Z2 = sess.run(W_conv2)\n",
    "    Z2 = projection(Z2, percent=P2)\n",
    "\n",
    "    U2 = np.zeros_like(Z2)\n",
    "\n",
    "    Z3 = sess.run(W_fc1)\n",
    "    Z3 = projection(Z3, percent=P3)\n",
    "\n",
    "    U3 = np.zeros_like(Z3)\n",
    "\n",
    "    Z4 = sess.run(W_fc2)\n",
    "    Z4 = projection(Z4, percent=P4)\n",
    "\n",
    "    U4 = np.zeros_like(Z4)\n",
    "\n",
    "    for j in range(10):\n",
    "        for i in range(1000):\n",
    "            batch = mnist.train.next_batch(50)\n",
    "            if i % 100 == 0:\n",
    "                train_accuracy = accuracy.eval(feed_dict={\n",
    "                    x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "                print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "            train_step1.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0, A:Z1, B:U1, C:Z2, D:U2, E:Z3, F:U3, G:Z4, H:U4})\n",
    "        Z1 = sess.run(W_conv1) + U1\n",
    "        Z1 = projection(Z1, percent=P1)\n",
    "\n",
    "        U1 = U1 + sess.run(W_conv1) - Z1\n",
    "\n",
    "        Z2 = sess.run(W_conv2) + U2\n",
    "        Z2 = projection(Z2, percent=P2)\n",
    "\n",
    "        U2 = U2 + sess.run(W_conv2) - Z2\n",
    "\n",
    "        Z3 = sess.run(W_fc1) + U3\n",
    "        Z3 = projection(Z3, percent=P3)\n",
    "\n",
    "        U3 = U3 + sess.run(W_fc1) - Z3\n",
    "\n",
    "        Z4 = sess.run(W_fc2) + U4\n",
    "        Z4 = projection(Z4, percent=P4)\n",
    "\n",
    "        U4 = U4 + sess.run(W_fc2) - Z4\n",
    "\n",
    "        print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "            x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "        print(LA.norm(sess.run(W_conv1) - Z1))\n",
    "        print(LA.norm(sess.run(W_conv2) - Z2))\n",
    "        print(LA.norm(sess.run(W_fc1) - Z3))\n",
    "        print(LA.norm(sess.run(W_fc2) - Z4))\n",
    "\n",
    "    dense_w['conv1/W_conv1'] = W_conv1\n",
    "    dense_w['conv2/W_conv2'] = W_conv2\n",
    "    dense_w['fc1/W_fc1'] = W_fc1\n",
    "    dense_w['fc2/W_fc2'] = W_fc2\n",
    "    \n",
    "    dict_nzidx = apply_prune(dense_w,sess)\n",
    "    print (\"checking space dictionary\")\n",
    "    print (dict_nzidx.keys())\n",
    "    grads = apply_prune_on_grads(grads,dict_nzidx)\n",
    "    apply_gradient_op = my_trainer.apply_gradients(grads)\n",
    "    for var in tf.global_variables():\n",
    "                if tf.is_variable_initialized(var).eval() == False:\n",
    "                    sess.run(tf.variables_initializer([var]))\n",
    "    print (\"start retraining after pruning\")\n",
    "    for i in range(2000):\n",
    "      batch = mnist.train.next_batch(50)\n",
    "      if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "            x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "\n",
    "      apply_gradient_op.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "          x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))\n",
    "    print(np.sum(sess.run(W_conv1)!=0))\n",
    "    print(np.sum(sess.run(W_conv2) != 0))\n",
    "    print(np.sum(sess.run(W_fc1) != 0))\n",
    "    print(np.sum(sess.run(W_fc2) != 0))\n",
    "    # do the saving.\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess,\"./lenet_5_pruned_model.ckpt\")\n",
    "if __name__ == '__main__':\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--data_dir', type=str,\n",
    "                      default='/tmp/tensorflow/mnist/input_data',\n",
    "                      help='Directory for storing input data')\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
